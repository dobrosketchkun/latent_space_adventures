{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Latent_space_projection_styleGAN2-ada-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_yuNNDOoL_q"
      },
      "source": [
        "# **How to use this notebook:**\n",
        "\n",
        "Using this colab, you can project photo portrait* into latent space.\n",
        "\n",
        "First, run 1. and 1.1 (this cell will print the list of available networks)   \n",
        "If you have saved state in npz - use 2.b, otherwise if you have an image, paste the path to the file (or url). Seed is used to randomize final result, which will be not particularly good most of the time, but it's ok. You can change it later.   \n",
        "On 3. you choose a network to secondary project onto. The result will be generally awful, that's why you need to generate a new seed to copy a style from. You can use a specific seed or keep it blank for a random seed. After preview, you can rerun 3.4 with checked \"save_projected\" to save an image.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "* you can technically try to project anything, but it's on you. In order to work around an alignment error if our image doesn't contain a face - just ignore it, past path to file (it's better be jpg) into _face_file_path_, run the cell, then rename your file \"original_name-aligned.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhAzW7zzprGm",
        "cellView": "form"
      },
      "source": [
        "#@title 1. Installs\n",
        "from IPython.display import clear_output \n",
        "%cd /content/\n",
        "!git clone https://github.com/NVlabs/stylegan2.git\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch/\n",
        "!pip install ninja\n",
        "\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "clear_output()\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MGLuubPrGOw",
        "cellView": "form"
      },
      "source": [
        "#@title # 1.1 Neural Networks handling\n",
        "%cd /content/\n",
        "\n",
        "import json\n",
        "\n",
        "!curl https://raw.githubusercontent.com/dobrosketchkun/wd_network_zoo/main/wd_networks.json -O wd_networks.json\n",
        "\n",
        "\n",
        "with open('/content/wd_networks.json', 'r') as f:\n",
        "    jnetw = json.loads(f.read())\n",
        "\n",
        "nn_list = list(jnetw.keys())\n",
        "\n",
        "network_name = '' #@#param {type:\"string\"}\n",
        "download_all = True #@#param {type:\"boolean\"}\n",
        "\n",
        "if download_all:\n",
        "    for nn in nn_list:\n",
        "        url = jnetw[nn]\n",
        "        id =  url.split('/')[-2]\n",
        "        network_f_name = nn + '.pkl'\n",
        "        !gdown --id $id -O $network_f_name\n",
        "    clear_output()\n",
        "    network_name = 'ffhq'\n",
        "    print('Network:', network_name)\n",
        "else:\n",
        "    if not network_name:\n",
        "        print('List of available networks:')\n",
        "        print(', '.join(nn_list))\n",
        "    elif network_name not in nn_list:\n",
        "        print('Error, You can not use {}! Only names from this list are available:'.format(network_name))\n",
        "        print(', '.join(nn_list))\n",
        "    else:\n",
        "        clear_output()\n",
        "        print('Network:', network_name)\n",
        "        url = jnetw[network_name]\n",
        "        id =  url.split('/')[-2]\n",
        "        network_f_name = network_name + '.pkl'\n",
        "        !gdown --id $id -O $network_f_name\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print('All networks are downloded.')\n",
        "print('List of available networks:')\n",
        "print(', '.join(nn_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kFntfGhqVu7",
        "cellView": "form"
      },
      "source": [
        "#@title # 2.a Alignment\n",
        "%cd /content/\n",
        "clear_output()\n",
        "\n",
        "face_file_path = 'https://hungarytoday.hu/wp-content/uploads/2020/06/Hide-the-Pain-Harold-prof..jpg' #@param {type:\"string\"}\n",
        "\n",
        "import os.path\n",
        "\n",
        "\n",
        "\n",
        "if os.path.isfile(face_file_path):\n",
        "    pass\n",
        "else:\n",
        "    face_file_path = '\"' + face_file_path + '\"'\n",
        "    !wget $face_file_path -O image.imgs\n",
        "    face_file_path = '/content/image.imgs'\n",
        "    # clear_output()\n",
        "\n",
        "\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "im = Image.open(face_file_path)\n",
        "rgb_im = im.convert('RGB')\n",
        "face_file_path = face_file_path.split('.')[0] + '.jpg'\n",
        "rgb_im.save(face_file_path, quality=90)#, subsampling=0)\n",
        "\n",
        "\n",
        "%cd /content/stylegan2\n",
        "%tensorflow_version 1.x\n",
        "# !nvcc test_nvcc.cu -o test_nvcc -run\n",
        "import sys\n",
        "import os\n",
        "import dlib\n",
        "import glob\n",
        "\n",
        "def detect_face_landmarks(face_file_path=None,\n",
        "                          predictor_path=None,\n",
        "                          img=None):\n",
        "  # References:\n",
        "  # -   http://dlib.net/face_landmark_detection.py.html\n",
        "  # -   http://dlib.net/face_alignment.py.html\n",
        "\n",
        "  if predictor_path is None:\n",
        "    predictor_path = '/content/shape_predictor_68_face_landmarks.dat'\n",
        "\n",
        "  # Load all the models we need: a detector to find the faces, a shape predictor\n",
        "  # to find face landmarks so we can precisely localize the face\n",
        "  detector = dlib.get_frontal_face_detector()\n",
        "  shape_predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "  if img is None:\n",
        "    # Load the image using Dlib\n",
        "    # print(\"Processing file: {}\".format(face_file_path))\n",
        "    img = dlib.load_rgb_image(face_file_path)\n",
        "\n",
        "  shapes = list()\n",
        "\n",
        "  # Ask the detector to find the bounding boxes of each face. The 1 in the\n",
        "  # second argument indicates that we should upsample the image 1 time. This\n",
        "  # will make everything bigger and allow us to detect more faces.\n",
        "  dets = detector(img, 1)\n",
        "    \n",
        "  num_faces = len(dets)\n",
        "#   print(\"Number of faces detected: {}\".format(num_faces))\n",
        "\n",
        "  # Find the face landmarks we need to do the alignment.\n",
        "  faces = dlib.full_object_detections()\n",
        "  for d in dets:\n",
        "    #   print(\"Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
        "    #       d.left(), d.top(), d.right(), d.bottom()\n",
        "    #   ))\n",
        "\n",
        "      shape = shape_predictor(img, d)\n",
        "      faces.append(shape)\n",
        "\n",
        "  return faces\n",
        "  \n",
        "  ################\n",
        "  \n",
        "faces = detect_face_landmarks(face_file_path=face_file_path)\n",
        "  \n",
        "  ################\n",
        "  \n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plot_style = dict(marker='o',\n",
        "                  markersize=4,\n",
        "                  linestyle='-',\n",
        "                  lw=2)\n",
        "\n",
        "pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\n",
        "pred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n",
        "              'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n",
        "              'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n",
        "              'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n",
        "              'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n",
        "              'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n",
        "              'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n",
        "              'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n",
        "              'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n",
        "              }\n",
        "\n",
        "\n",
        "\n",
        "def display_landmarks_raw(input_img, preds=None, fig_size=None):\n",
        "  # This is a raw copy from:\n",
        "  # https://github.com/1adrianb/face-alignment/blob/master/examples/detect_landmarks_in_image.py\n",
        "\n",
        "  if fig_size is None:\n",
        "    fig_size = plt.figaspect(.5)\n",
        "\n",
        "  fig = plt.figure(figsize=fig_size)\n",
        "  ax = fig.add_subplot(1, 1, 1) # only display one image\n",
        "  ax.imshow(input_img)\n",
        "\n",
        "  if preds is not None:\n",
        "    for pred_type in pred_types.values():\n",
        "        ax.plot(preds[pred_type.slice, 0],\n",
        "                preds[pred_type.slice, 1],\n",
        "                color=pred_type.color, **plot_style)\n",
        "\n",
        "  ax.axis('off')\n",
        "\n",
        "  return\n",
        "\n",
        "  import numpy as np\n",
        "from skimage import io\n",
        "\n",
        "def display_landmarks(image_name, \n",
        "                      dlib_output_faces=None, \n",
        "                      face_no=0,\n",
        "                      fig_size=None):\n",
        "  \n",
        "  if fig_size is None:\n",
        "    fig_size = [15, 15]\n",
        "\n",
        "  input_img = io.imread(image_name)\n",
        "\n",
        "  if dlib_output_faces is None:\n",
        "    dlib_output_faces = detect_face_landmarks(face_file_path=image_name,\n",
        "                                              img=input_img)\n",
        "\n",
        "  try:\n",
        "    current_face = dlib_output_faces[face_no]\n",
        "\n",
        "  except IndexError:\n",
        "    current_face = None\n",
        "\n",
        "    print('No face found for index n°{} (max={}).'.format(\n",
        "        face_no, \n",
        "        len(dlib_output_faces)-1,\n",
        "        ))\n",
        "\n",
        "  if current_face is None:\n",
        "    preds = None\n",
        "  else:\n",
        "    face_parts = current_face.parts()\n",
        "    \n",
        "    preds = np.array([\n",
        "                      [v.x, v.y] \n",
        "                      for v in face_parts\n",
        "                      ])    \n",
        "    \n",
        "  display_landmarks_raw(input_img=input_img, \n",
        "                        preds=preds,\n",
        "                        fig_size=fig_size)  \n",
        "\n",
        "  return\n",
        "\n",
        "face_no=0\n",
        "fig_size=[15,15]\n",
        "\n",
        "display_landmarks(image_name=face_file_path,\n",
        "                  dlib_output_faces=faces,\n",
        "                  face_no=face_no,\n",
        "                  fig_size=fig_size)\n",
        "\n",
        "\n",
        "################\n",
        "ffhq_aligned_image_name = face_file_path.split('.')[0] + '-aligned.jpg'\n",
        "################\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import html\n",
        "import hashlib\n",
        "import PIL.Image\n",
        "import PIL.ImageFile\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "import json\n",
        "import uuid\n",
        "import glob\n",
        "import argparse\n",
        "import itertools\n",
        "import shutil\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "# Reference: https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
        "\n",
        "def recreate_aligned_images(json_data, \n",
        "                            dst_dir='realign1024x1024',\n",
        "                            output_size=1024, \n",
        "                            transform_size=4096, \n",
        "                            enable_padding=True):\n",
        "    # print('Recreating aligned images...')\n",
        "    if dst_dir:\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "    for item_idx, item in enumerate(json_data.values()):\n",
        "        # print('\\r%d / %d ... ' % (item_idx, len(json_data)), end='', flush=True)\n",
        "\n",
        "        # Parse landmarks.\n",
        "        # pylint: disable=unused-variable\n",
        "        lm = np.array(item['in_the_wild']['face_landmarks'])\n",
        "        lm_chin          = lm[0  : 17]  # left-right\n",
        "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "        lm_nose          = lm[27 : 31]  # top-down\n",
        "        lm_nostrils      = lm[31 : 36]  # top-down\n",
        "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "        # Calculate auxiliary vectors.\n",
        "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "        eye_avg      = (eye_left + eye_right) * 0.5\n",
        "        eye_to_eye   = eye_right - eye_left\n",
        "        mouth_left   = lm_mouth_outer[0]\n",
        "        mouth_right  = lm_mouth_outer[6]\n",
        "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "        eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "        # Choose oriented crop rectangle.\n",
        "        # print(eye_to_mouth.shape)\n",
        "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "        x /= np.hypot(*x)\n",
        "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "        y = np.flipud(x) * [-1, 1]\n",
        "        c = eye_avg + eye_to_mouth * 0.1\n",
        "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "        qsize = np.hypot(*x) * 2\n",
        "\n",
        "        # Load in-the-wild image.\n",
        "        src_file = item['in_the_wild']['file_path']\n",
        "        img = PIL.Image.open(src_file)\n",
        "\n",
        "        # Shrink.\n",
        "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "        if shrink > 1:\n",
        "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "            quad /= shrink\n",
        "            qsize /= shrink\n",
        "\n",
        "        # Crop.\n",
        "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "            img = img.crop(crop)\n",
        "            quad -= crop[0:2]\n",
        "\n",
        "        # Pad.\n",
        "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "        if enable_padding and max(pad) > border - 4:\n",
        "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "            h, w, _ = img.shape\n",
        "            y, x, _ = np.ogrid[:h, :w, :1]\n",
        "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "            blur = qsize * 0.02\n",
        "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "            quad += pad[:2]\n",
        "\n",
        "        # Transform.\n",
        "        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "        if output_size < transform_size:\n",
        "            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "        # Save aligned image.\n",
        "        dst_subdir = os.path.join(dst_dir, '%05d' % (item_idx - item_idx % 1000))\n",
        "        os.makedirs(dst_subdir, exist_ok=True)\n",
        "        img.save(os.path.join(dst_subdir, '%05d.png' % item_idx))\n",
        "\n",
        "    # All done.\n",
        "    # print('\\r%d / %d ... done' % (len(json_data), len(json_data)))\n",
        "\n",
        "    return\n",
        "\n",
        "################\n",
        "\n",
        "# The first face which is detected:\n",
        "# NB: we assume that there is exactly one face per picture!\n",
        "f = faces[0]\n",
        "\n",
        "parts = f.parts()\n",
        "\n",
        "num_face_landmarks=68\n",
        "\n",
        "v = np.zeros(shape=(num_face_landmarks, 2))\n",
        "for k, e in enumerate(parts):\n",
        "  v[k, :] = [e.x, e.y]\n",
        "\n",
        "\n",
        "json_data = dict()\n",
        "\n",
        "item_idx = 0\n",
        "\n",
        "json_data[item_idx] = dict()\n",
        "json_data[item_idx]['in_the_wild'] = dict()\n",
        "json_data[item_idx]['in_the_wild']['file_path'] = face_file_path\n",
        "json_data[item_idx]['in_the_wild']['face_landmarks'] = v\n",
        "\n",
        "recreate_aligned_images(json_data)\n",
        "\n",
        "!cp '/content/stylegan2/realign1024x1024/00000/00000.png' $ffhq_aligned_image_name\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJocaqRtsbbg",
        "cellView": "form"
      },
      "source": [
        "#@title # 2.a.1 Projecting\n",
        "%cd /content/stylegan2-ada-pytorch/\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "from typing import List, Optional\n",
        "import click\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from math import ceil\n",
        "import argparse\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "out_dir = '/content/out'\n",
        "save_projected_w = True #@param {type:\"boolean\"}\n",
        "# save_video = str(save_video)\n",
        "network_name_ = '/content/' + network_name + '.pkl'\n",
        "\n",
        "seed = '' #@param {type:\"string\"}\n",
        "\n",
        "# /content/ffhq.pkl\n",
        "\n",
        "if seed:\n",
        "    pass\n",
        "else:\n",
        "    seed = np.random.randint(2**32-1)\n",
        "\n",
        "!python projector.py --outdir=$out_dir \\\n",
        "    --target=/$ffhq_aligned_image_name --num-steps=800\\\n",
        "    --network=$network_name_ --save-video=False --seed=410\n",
        "\n",
        "clear_output()\n",
        "\n",
        "def createImageGrid(images, scale=0.25, rows=1):\n",
        "  #  PIL.Image\n",
        "   w,h = images[0].size\n",
        "   w = int(w*scale)\n",
        "   h = int(h*scale)\n",
        "   height = rows*h\n",
        "   cols = ceil(len(images) / rows)\n",
        "   width = cols*w\n",
        "   canvas = PIL.Image.new('RGBA', (width,height), 'white')\n",
        "   for i,img in enumerate(images):\n",
        "     img = img.resize((w,h), PIL.Image.ANTIALIAS)\n",
        "     canvas.paste(img, (w*(i % cols), h*(i // cols))) \n",
        "   return canvas\n",
        "\n",
        "\n",
        "if save_projected_w:\n",
        "    files.download('{}/projected_w.npz'.format(out_dir))\n",
        "\n",
        "orig = PIL.Image.open('{}/target.png'.format(out_dir))\n",
        "proj = PIL.Image.open('{}/proj.png'.format(out_dir))\n",
        "display(createImageGrid(images=[orig, proj], scale=0.5, rows=1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "769ps5qMRjR1"
      },
      "source": [
        "#@title # 2.b If you have a projected_w.npz file\n",
        "%cd /content/\n",
        "\n",
        "\n",
        "out_dir = '/content/out'\n",
        "\n",
        "!mkdir $out_dir\n",
        "\n",
        "projected_w = '' #@param {type:\"string\"}\n",
        "\n",
        "!cp $projected_w /content/out/projected_w.npz\n",
        "\n",
        "#@title # Load style network\n",
        "\n",
        "%cd /content/stylegan2-ada-pytorch/\n",
        "import dnnlib\n",
        "import legacy\n",
        "import os\n",
        "import re\n",
        "from typing import List, Optional\n",
        "import click\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from math import ceil\n",
        "import argparse\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def createImageGrid(images, scale=0.25, rows=1):\n",
        "  #  PIL.Image\n",
        "   w,h = images[0].size\n",
        "   w = int(w*scale)\n",
        "   h = int(h*scale)\n",
        "   height = rows*h\n",
        "   cols = ceil(len(images) / rows)\n",
        "   width = cols*w\n",
        "   canvas = PIL.Image.new('RGBA', (width,height), 'white')\n",
        "   for i,img in enumerate(images):\n",
        "     img = img.resize((w,h), PIL.Image.ANTIALIAS)\n",
        "     canvas.paste(img, (w*(i % cols), h*(i // cols))) \n",
        "   return canvas\n",
        "\n",
        "\n",
        "\n",
        "network_name_ = '' #@param {type: \"string\"}\n",
        "\n",
        "network_pkl = '/content/' + network_name_ + '.pkl'\n",
        "noise_mode = 'const'\n",
        "\n",
        "\n",
        "# print('Loading networks from \"%s\"...' % network_pkl)\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "\n",
        "\n",
        "\n",
        "ws = np.load(projected_w)['w']\n",
        "ws = torch.tensor(ws, device=device) \n",
        "all_w = ws\n",
        "orig_im_all_images = G.synthesis(all_w, noise_mode=noise_mode)\n",
        "orig_im_all_images = (orig_im_all_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
        "orig_im = PIL.Image.fromarray(orig_im_all_images[0], 'RGB') \n",
        "orig_im.save('{}/target.png'.format(out_dir))#, subsampling=0)\n",
        "\n",
        "display(createImageGrid([orig_im], scale=0.5, rows=1))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20WYjl1u5LRS"
      },
      "source": [
        "# 3. Copy style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0I28AiwzJCa",
        "cellView": "form"
      },
      "source": [
        "#@title # 3.1 Secondary network preview \n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "print('List of available networks:')\n",
        "print(', '.join(nn_list))\n",
        "\n",
        "\n",
        "network_name_project = '' #@param {type:\"string\"}\n",
        "if network_name_project:\n",
        "    projected_w = out_dir + '/projected_w.npz'\n",
        "    out_dir2_dump = '/content/projected/'\n",
        "    out_dir2 = '/content/proj_temp/'\n",
        "\n",
        "    Path(out_dir2).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    network_name_ = '/content/' + network_name_project + '.pkl'\n",
        "\n",
        "    print(network_name_)\n",
        "\n",
        "    %cd /content/stylegan2-ada-pytorch/\n",
        "    !python generate.py --outdir=$out_dir2 --projected-w=$projected_w \\\n",
        "        --network=$network_name_\n",
        "\n",
        "    orig = PIL.Image.open('{}/target.png'.format(out_dir))\n",
        "    proj = PIL.Image.open('{}/proj00.png'.format(out_dir2))\n",
        "    display(createImageGrid(images=[orig, proj], scale=0.5, rows=1))\n",
        "\n",
        "    \n",
        "else:\n",
        "    raise Exception('Choose some network to project on.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhLYkpM75HKy",
        "cellView": "form"
      },
      "source": [
        "#@title # 3.2 Load style network\n",
        "%cd /content/stylegan2-ada-pytorch/\n",
        "\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "network_pkl = network_name_\n",
        "noise_mode = 'const'\n",
        "\n",
        "\n",
        "# print('Loading networks from \"%s\"...' % network_pkl)\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IZjII0s5UU-",
        "cellView": "form"
      },
      "source": [
        "#@title # 3.3 Choose the style\n",
        "import numpy as np\n",
        "\n",
        "seed = '' #@param {type:\"string\"}\n",
        "try:\n",
        "    seed = np.array([int(seed)])\n",
        "except Exception:\n",
        "    pass\n",
        "if not seed:\n",
        "    seed = np.random.randint(2**32-1, size=1)\n",
        "print('Seed of this image is:', seed[0])\n",
        "\n",
        "truncation_psi = 0.7\n",
        "rand = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "all_z = np.stack(rand)\n",
        "all_w = G.mapping(torch.from_numpy(all_z).to(device), None)\n",
        "w_avg = G.mapping.w_avg\n",
        "all_w = w_avg + (all_w - w_avg) * truncation_psi\n",
        "all_images = G.synthesis(all_w, noise_mode=noise_mode)\n",
        "all_images = (all_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
        "im = PIL.Image.fromarray(all_images[0], 'RGB')\n",
        "createImageGrid([im], scale=0.5, rows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Wiu8Wj6Lg0",
        "cellView": "form"
      },
      "source": [
        "#@title # 3.4 Preview\n",
        "import uuid\n",
        "\n",
        "deeper_copy = True #@param {type:\"boolean\"}\n",
        "even_deeper_copy = False #@param {type:\"boolean\"}\n",
        "save_projected_image = True #@param {type:\"boolean\"}\n",
        "save_npz = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "ws = np.load(projected_w)['w']\n",
        "ws = torch.tensor(ws, device=device) \n",
        "if even_deeper_copy:\n",
        "    ws[0][6] = all_w[0][6]\n",
        "if deeper_copy:\n",
        "    ws[0][7] = all_w[0][7]\n",
        "ws[0][8] = all_w[0][8]\n",
        "ws[0][9] = all_w[0][9]\n",
        "ws[0][10] = all_w[0][10]\n",
        "ws[0][11] = all_w[0][11]\n",
        "ws[0][12] = all_w[0][12]\n",
        "ws[0][13] = all_w[0][13]\n",
        "ws[0][14] = all_w[0][14]\n",
        "ws[0][15] = all_w[0][15]\n",
        "ws[0][16] = all_w[0][16]\n",
        "ws[0][17] = all_w[0][17]\n",
        "all_images = G.synthesis(ws, noise_mode=noise_mode)\n",
        "all_images = (all_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
        "im = PIL.Image.fromarray(all_images[0], 'RGB')\n",
        "\n",
        "# createImageGrid([im], scale=0.5, rows=1)\n",
        "\n",
        "orig = PIL.Image.open('{}/target.png'.format(out_dir))\n",
        "proj = im\n",
        "display(createImageGrid(images=[orig, proj], scale=0.5, rows=1))\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "out_dir2_dump = '/content/projected/'\n",
        "Path(out_dir2_dump).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if save_projected_image:\n",
        "    filename = network_name_project + '_' + uuid.uuid4().hex + '.jpg'\n",
        "    im.save(out_dir2_dump +  filename, quality=90)#, subsampling=0)\n",
        "\n",
        "if save_npz:\n",
        "    np.savez('/content/' + uuid.uuid4().hex + '_projected_w_ns.npz', w=ws)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMSN9Kf_T7N",
        "cellView": "form"
      },
      "source": [
        "#@title # 4. Preview all saved images\n",
        "import glob\n",
        "import random\n",
        "\n",
        "%cd $out_dir2_dump\n",
        "images = glob.glob('*.jpg')\n",
        "images = list(sorted(images)[::-1])\n",
        "\n",
        "size = len(images)\n",
        "in_a_row =  4#@param  {type:\"integer\"}\n",
        "scale = 0.4 #@param  {type:\"number\"}\n",
        "shuffle = True #@param {type:\"boolean\"}\n",
        "if shuffle:\n",
        "   random.shuffle(images)\n",
        "\n",
        "images = [orig] + [PIL.Image.open(_) for _ in images]\n",
        "\n",
        "\n",
        "rows = ceil(size/in_a_row)\n",
        "\n",
        "\n",
        "display(createImageGrid(images=images, scale=scale, rows=rows))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}